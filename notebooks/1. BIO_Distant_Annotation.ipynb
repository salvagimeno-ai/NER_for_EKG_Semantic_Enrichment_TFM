{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIO Distant Annotation file from entity list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THis notebook produces the BIO Annotation file using string matching, based on a given entity list or gazette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Entity list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAP PRODUCTS LIST file:\n",
    "PRODUCTS_FOLDER = \"sap-products/\"\n",
    "sap_products_file = \"prod_full_sap-L1_v2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Text corpus: in this case from SAP Corp-Wiki dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"corp_wiki_raw_texts/\"\n",
    "max_len = 256\n",
    "file_from = 151\n",
    "file_to = 161\n",
    "file_number = np.arange(file_from, file_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd                #Pandas - Data processing, data frames\n",
    "pd.options.display.max_rows = 1000 #handy for showing truncated results\n",
    "import matplotlib.pyplot as plt    #matplotlib - graficas\n",
    "import numpy as np                 #Numpy - arrays y manejo de datos\n",
    "# import seaborn as sns              #Seaborn - Data visualization\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy \n",
    "!pip install -U spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import load\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.gold import biluo_tags_from_offsets\n",
    "from spacy.training import offsets_to_biluo_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD SAP-PRODUCTS (execute once) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD-ONCE:\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) LOAD SAP-PRODUCT_ENTITIES:\n",
    "# -------------------------------------------------------------------------\n",
    "# PRODUCTS_FOLDER = \"sap-products/\"\n",
    "# sap_products_file = \"prod_full_sap-L1_v2.csv\"\n",
    "\n",
    "sap_ent_prod =[]\n",
    "fileObject = open(PRODUCTS_FOLDER+sap_products_file, \"r\")\n",
    "sap_ent_prod = fileObject.read()\n",
    "sap_ent_prod = sap_ent_prod.split('\\n')\n",
    "# sap_ent_prod\n",
    "\n",
    "# Convert to lower case:\n",
    "sap_ent_prod_lwr = []\n",
    "for p in sap_ent_prod:\n",
    "    p = p.replace('\\ufeff','')\n",
    "    sap_ent_prod_lwr.append(p.lower())\n",
    "\n",
    "sap_ent_prod_lwr = sorted(set(sap_ent_prod_lwr))\n",
    "    \n",
    "print(len(sap_ent_prod_lwr))    \n",
    "sap_ent_prod_lwr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sap_ent_prod_lwr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TEXT (for unit testing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE_FOLDER =  \"corp_wiki_raw_texts/\"\n",
    "# !ls = \"{FILE_FOLDER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOLDER = \"corp_wiki_raw_texts/\"\n",
    "# file_path_from = FOLDER+\"corp-wiki-20\"\n",
    "# print('loading text from: ', file_path_from)\n",
    "# fileObject = open(file_path_from, \"r\")\n",
    "# text_string = fileObject.read()\n",
    "# text_string = text_string.lower()\n",
    "# text_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING FUNCTIONS (v.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# create function to remove numbers in brackets from text_string\n",
    "# -------------------------------------------------------------------------\n",
    "def generate_numbers_in_brackets_list(n=200):\n",
    "    numbers_list = np.arange(1,n)\n",
    "    brackets_numbers_list = []\n",
    "    for i in numbers_list:\n",
    "        string = '[' + str(i) + ']'\n",
    "        brackets_numbers_list.append(string)\n",
    "    return brackets_numbers_list\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# create function to replace a string with another string\n",
    "# -------------------------------------------------------------------------\n",
    "def replace_string_from_texts_list(string, strings_list, replace_by):\n",
    "    for s in strings_list:\n",
    "        new_string = string.replace(s, replace_by)\n",
    "    return new_string\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# SPLIT DATA_STRING INTO SENTENCES\n",
    "# -------------------------------------------------------------------------\n",
    "def split_text_string_into_sentences_v1(text_string, max_len):\n",
    "    text_sentences = []\n",
    "    text_sentences_tmp = text_string.split('.')\n",
    "    strg_carry_forward = '' \n",
    "    \n",
    "    for s_ in text_sentences_tmp:\n",
    "        s = strg_carry_forward + ' ' + s_\n",
    "        nr_wrds = s.split()\n",
    "        if len(nr_wrds) > 5:\n",
    "            if len(s) <= max_len:\n",
    "                s_nlts = s.strip()\n",
    "                s_nlts = s_nlts.replace('     ',' ') # replace 5 spaces by 1 space\n",
    "                s_nlts = s_nlts.replace('    ',' ') # replace 4 spaces by 1 space\n",
    "                s_nlts = s_nlts.replace('   ',' ') # replace 3 spaces by 1 space\n",
    "                s_nlts = s_nlts.replace('  ',' ') # replace 2 spaces by 1 space\n",
    "                text_sentences.append(s_nlts)\n",
    "                strg_carry_forward = ''\n",
    "        \n",
    "            else:  # s > max_len\n",
    "                #truncated 1\n",
    "                trunc1 = smart_truncate(s, length=max_len, suffix='')\n",
    "\n",
    "                s_nlts = trunc1.strip()\n",
    "                s_nlts = s_nlts.replace('     ',' ') # replace 5 spaces by 1 space\n",
    "                s_nlts = s_nlts.replace('    ',' ') # replace 4 spaces by 1 space\n",
    "                s_nlts = s_nlts.replace('   ',' ') # replace 3 spaces by 1 space\n",
    "                s_nlts = s_nlts.replace('  ',' ') # replace 2 spaces by 1 space\n",
    "                text_sentences.append(s_nlts)\n",
    "                \n",
    "                # truncated 2\n",
    "                index_trunc1 = len(trunc1)\n",
    "                trunc2 = s[index_trunc1:]\n",
    "                nr_wrds = trunc2.split()\n",
    "                if len(nr_wrds) > 5:\n",
    "                    s_nlts = trunc2.strip()\n",
    "                    s_nlts = s_nlts.replace('     ',' ') # replace 5 spaces by 1 space\n",
    "                    s_nlts = s_nlts.replace('    ',' ') # replace 4 spaces by 1 space\n",
    "                    s_nlts = s_nlts.replace('   ',' ') # replace 3 spaces by 1 space\n",
    "                    s_nlts = s_nlts.replace('  ',' ') # replace 2 spaces by 1 space\n",
    "                    text_sentences.append(s_nlts)\n",
    "\n",
    "                    strg_carry_forward = ''\n",
    "                else:\n",
    "                    strg_carry_forward = ''\n",
    "        else:\n",
    "            strg_carry_forward = s\n",
    "            \n",
    "    return text_sentences\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# TRUNCATE A STRING WITHOUT BREAKING A WORD IN THE MODDLE\n",
    "# -------------------------------------------------------------------------\n",
    "def smart_truncate(content, length=100, suffix=''):\n",
    "    if len(content) <= length:\n",
    "        return content\n",
    "    else:\n",
    "        return ' '.join(content[:length+1].split(' ')[0:-1]) + suffix\n",
    "    \n",
    "# -------------------------------------------------------------------------\n",
    "# RESET SENTENCE NUMBERING OF ANNOTATED SENTENCES DATAFRAME\n",
    "# -------------------------------------------------------------------------\n",
    "def reset_annotated_file_sentence_numbering(df_data, n):\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "    previous_sentence = ''\n",
    "    counter = 1\n",
    "    sentence_counter = 'Sentence: '+str(n)+'_'+str(counter)\n",
    "\n",
    "    for i in df_data.index: \n",
    "        if i == 0:\n",
    "            previous_sentence = df_data['Sentence'][i]\n",
    "            df_data['Sentence'][i] = 'Sentence: '+str(n)+'_1'\n",
    "\n",
    "        else:\n",
    "            if df_data['Sentence'][i] == previous_sentence:\n",
    "                previous_sentence = df_data['Sentence'][i]\n",
    "                df_data['Sentence'][i] = sentence_counter\n",
    "            else:\n",
    "                counter = counter + 1\n",
    "                sentence_counter = 'Sentence: '+str(n)+'_'+str(counter)\n",
    "                previous_sentence = df_data['Sentence'][i]\n",
    "                df_data['Sentence'][i] = sentence_counter\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANING FUNCTIONS v3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_truncate(content, length=100, suffix=''):\n",
    "    if len(content) <= length:\n",
    "        return content\n",
    "    else:\n",
    "        return ' '.join(content[:length+1].split(' ')[0:-1]) + suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(s, max_len):\n",
    "    trunc1 = smart_truncate(s, length=max_len, suffix='')\n",
    "    s_nlts = trunc1.strip()\n",
    "    s_nlts = s_nlts.replace('     ',' ') # replace 5 spaces by 1 space\n",
    "    s_nlts = s_nlts.replace('    ',' ') # replace 4 spaces by 1 space\n",
    "    s_nlts = s_nlts.replace('   ',' ') # replace 3 spaces by 1 space\n",
    "    s_nlts = s_nlts.replace('  ',' ') # replace 2 spaces by 1 space\n",
    "#     text_sentences.append(s_nlts)\n",
    "     # truncated 2\n",
    "    index_trunc1 = len(trunc1)\n",
    "    trunc2 = s[index_trunc1:]\n",
    "    return s_nlts, trunc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNIT TESTING:\n",
    "# s = 'the company says that the combination of sap cloud platform with sap business technology platform is all about connecting business processes and experiences so asug members can make confident decisions with integrity'\n",
    "# print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## UNIT TESTING: TRUNCATE function\n",
    "# max_len = 300\n",
    "\n",
    "# tr1, tr2 = truncate(s, max_len)\n",
    "# print('tr1: ', tr1)\n",
    "# print(len(tr1))\n",
    "# print('tr2: ', tr2)\n",
    "# print(len(tr2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_spaces(s):\n",
    "    s_nlts = s.strip()\n",
    "    s_nlts = s_nlts.replace('     ',' ') # replace 5 spaces by 1 space\n",
    "    s_nlts = s_nlts.replace('    ',' ') # replace 4 spaces by 1 space\n",
    "    s_nlts = s_nlts.replace('   ',' ') # replace 3 spaces by 1 space\n",
    "    s_nlts = s_nlts.replace('  ',' ') # replace 2 spaces by 1 space\n",
    "    return s_nlts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_string_into_sentences_v2(text_string, max_len):\n",
    "    text_sentences = []\n",
    "    text_sentences_tmp = text_string.split('.')\n",
    "    strg_carry_forward = '' \n",
    "    \n",
    "    for s_ in text_sentences_tmp:\n",
    "        s = strg_carry_forward + ' ' + s_\n",
    "#         print(s)\n",
    "        \n",
    "        nr_wrds = s.split()\n",
    "        \n",
    "        if len(nr_wrds) > 5:\n",
    "            if len(s) <= max_len: \n",
    "#                 print(len(s), ' <= ' , max_len)\n",
    "                s = eliminate_spaces(s)\n",
    "                text_sentences.append(s)\n",
    "                strg_carry_forward = ''\n",
    "            else:\n",
    "#                 print(len(s), ' > ' , max_len)\n",
    "            \n",
    "                counter = 1\n",
    "                while len(s) > max_len:\n",
    "#                     print(counter)\n",
    "                    tr1, tr2 = truncate(s, max_len)\n",
    "                    tr1 = eliminate_spaces(tr1)\n",
    "                    text_sentences.append(tr1)\n",
    "\n",
    "                    s = tr2\n",
    "#                     print('tr1: ', tr1)\n",
    "#                     print('tr2: ', tr2)\n",
    "#                     print('s: ', s)\n",
    "                    \n",
    "                if len(s) > 0 and len(s) < max_len:\n",
    "#                     print('len(s) > 0 and len(s) < max_len:')\n",
    "                    nr_wrds = s.split()\n",
    "                    if len(nr_wrds) > 5:\n",
    "                        s = eliminate_spaces(s)\n",
    "                        text_sentences.append(s)\n",
    "                        strg_carry_forward = ''\n",
    "                    else:\n",
    "                        strg_carry_forward = s\n",
    "                counter = counter + 1\n",
    "        else:\n",
    "#             print('s has less than 5 words')\n",
    "            strg_carry_forward = s\n",
    "    \n",
    "    if len(strg_carry_forward) > 0:\n",
    "        strg_carry_forward = eliminate_spaces(strg_carry_forward)\n",
    "        text_sentences.append(strg_carry_forward)\n",
    "        \n",
    "    \n",
    "    return text_sentences\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNIT TESTING: split_text_string_into_sentences_v3\n",
    "# s = 'the company says that the combination of sap cloud platform with sap business technology platform is all about connecting business processes and experiences so asug members can make confident decisions with integrity. as sap gold partner delaware consulting and sap work together to implement s/4hana cloud for rehau which is the first s/4hana live customer in china.'\n",
    "# # s = 'the company says that'\n",
    "# print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UNIT TESTING: split_text_string_into_sentences_v3\n",
    "# max_len = 128\n",
    "\n",
    "# split_text_string_into_sentences_v2(s, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New function to clean text and break into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# CLEAN TEXT AND BREAK INTO SENTENCES\n",
    "# -------------------------------------------------------------------------\n",
    "def clean_text_string_and_break_into_sentences_v1(text_string, max_len):\n",
    "    s = text_string.lower()\n",
    "\n",
    "    s = re.sub(\"@\\S+\", \" \", s)    #remove twitter account\n",
    "    s = re.sub(\"https*\\S+\", \" \", s)  #remove url\n",
    "    s = s.replace(\"\\t\", \" \")  # remove tabs\n",
    "    s = s.replace(\"\\n\",\". \")  # remove line-breaks\n",
    "\n",
    "    # DELETE BEGGINING_OF_ARTICLE NUMBER STRING by DOT SPACE\n",
    "    # s = \"1965636085\\ This must not b3 delet3d, but the number at the end yes 134411 the date of release was 23 March 2019\\n2268135735\\tthe following are the test cases/browser/platform combinations that were run.\"\n",
    "    s = s.replace(\"\\ \", \" \")\n",
    "    regex = re.compile(r'\\d{10}')\n",
    "    numbers_to_delete = regex.findall(s)\n",
    "    numbers_to_delete\n",
    "    for i in numbers_to_delete:\n",
    "        s = s.replace(str(i), \". \")\n",
    "\n",
    "    # DELETE VERSION NUMBERS\n",
    "    # s = \"sap hana 2.0 is the mew version. sap s/4hana 15.11 is the latest s4 version.\"                  \n",
    "    regex = re.compile(r'\\b\\d+\\.\\d+\\b')\n",
    "    version_numbers = regex.findall(s)\n",
    "    #version_numbers\n",
    "\n",
    "    for i in version_numbers:\n",
    "        s = s.replace(str(i), \" \")\n",
    "\n",
    "    # REMOVE / EXCEPT FROM IN S/4HANA\n",
    "    # s = \"sap hana is the in-memory database. sap s/4hana is the latest sap erp version. the test cases/browser/platform combinations that will be run\"\n",
    "    s = s.replace(\"s/4hana\", \"s4-hana\")\n",
    "    s = s.replace(\"s/4\", \"s-4\")\n",
    "    s = s.replace(\"/\", \" \")\n",
    "    s = s.replace(\"s4-hana\", \"s4/hana\")\n",
    "    s = s.replace(\"s-4\", \"s/4\")\n",
    "\n",
    "    # OTHER CHARACTER REPLACEMENTS\n",
    "    s = s.replace(\"...\",\". \")  # remove line-breaks\n",
    "    s = s.replace(\". ..\",\". \") # remove line-breaks\n",
    "    s = s.replace(\":\",\" \")     # remove line-breaks\n",
    "    s = re.sub(\"#\\S+\", \" \", s) # Remove hashtags\n",
    "    s = re.sub(\"\\'\\w+\", '', s) # Remove ticks and the next character\n",
    "\n",
    "    brackets_numbers_list = generate_numbers_in_brackets_list(n=200)\n",
    "    s = replace_string_from_texts_list(s, brackets_numbers_list,'')\n",
    "\n",
    "    s = re.sub('[^A-Za-z0-9.]+', ' ', s)  # Remove all punctuations, except from .\n",
    "    s = re.sub(r'\\w*\\d+\\w*', '', s)       # Remove digits\n",
    "\n",
    "    #replace strings\n",
    "    s = s.replace(\"i.e\", \" \")  # remove i.e\n",
    "    s = s.replace(\"run.\", \" \")  # remove run.\n",
    "    s = s.replace(\".zip\", \" \")  # remove .zip\n",
    "    s = s.replace(\"server.xml\", \" \")  # remove server.xml\n",
    "    s = s.replace(\".yml\", \" \")  # remove .yml\n",
    "    s = s.replace(\".pai\", \" \")  # remove .pai\n",
    "    s = s.replace(\".xml\", \" \")  # remove .xml\n",
    "    s = s.replace(\".run\", \" \")  # remove .rum\n",
    "    s = s.replace(\".com\", \" \")  # remove .com\n",
    "\n",
    "    s = re.sub('\\s{2,}', \" \", s)  # Replace the over spaces\n",
    "\n",
    "    text_sentences = split_text_string_into_sentences_v1(s, max_len)\n",
    "    \n",
    "    return text_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# CLEAN TEXT AND BREAK INTO SENTENCES v2.0\n",
    "# -------------------------------------------------------------------------\n",
    "def clean_text_string_and_break_into_sentences_v2(text_string, max_len):\n",
    "    s = text_string.lower()\n",
    "\n",
    "    s = re.sub(\"@\\S+\", \" \", s)    #remove twitter account\n",
    "    s = re.sub(\"https*\\S+\", \" \", s)  #remove url\n",
    "    s = s.replace(\"\\t\", \" \")  # remove tabs\n",
    "    s = s.replace(\"\\n\",\". \")  # remove line-breaks\n",
    "\n",
    "    # DELETE BEGGINING_OF_ARTICLE NUMBER STRING by DOT SPACE\n",
    "    # s = \"1965636085\\ This must not b3 delet3d, but the number at the end yes 134411 the date of release was 23 March 2019\\n2268135735\\tthe following are the test cases/browser/platform combinations that were run.\"\n",
    "    s = s.replace(\"\\ \", \" \")\n",
    "    regex = re.compile(r'\\d{10}')\n",
    "    numbers_to_delete = regex.findall(s)\n",
    "    numbers_to_delete\n",
    "    for i in numbers_to_delete:\n",
    "        s = s.replace(str(i), \". \")\n",
    "\n",
    "    # DELETE VERSION NUMBERS\n",
    "    # s = \"sap hana 2.0 is the mew version. sap s/4hana 15.11 is the latest s4 version.\"                  \n",
    "    regex = re.compile(r'\\b\\d+\\.\\d+\\b')\n",
    "    version_numbers = regex.findall(s)\n",
    "    #version_numbers\n",
    "\n",
    "    for i in version_numbers:\n",
    "        s = s.replace(str(i), \" \")\n",
    "\n",
    "    # REMOVE / EXCEPT FROM IN S/4HANA\n",
    "    # s = \"sap hana is the in-memory database. sap s/4hana is the latest sap erp version. the test cases/browser/platform combinations that will be run\"\n",
    "    s = s.replace(\"s/4hana\", \"s4-hana\")\n",
    "    s = s.replace(\"s/4\", \"s-4\")\n",
    "    s = s.replace(\"/\", \" \")\n",
    "    s = s.replace(\"s4-hana\", \"s4/hana\")\n",
    "    s = s.replace(\"s-4\", \"s/4\")\n",
    "\n",
    "    # OTHER CHARACTER REPLACEMENTS\n",
    "    s = s.replace(\"...\",\". \")  # remove line-breaks\n",
    "    s = s.replace(\". ..\",\". \") # remove line-breaks\n",
    "    s = s.replace(\":\",\" \")     # remove line-breaks\n",
    "    s = re.sub(\"#\\S+\", \" \", s) # Remove hashtags\n",
    "    s = re.sub(\"\\'\\w+\", '', s) # Remove ticks and the next character\n",
    "\n",
    "    brackets_numbers_list = generate_numbers_in_brackets_list(n=200)\n",
    "    s = replace_string_from_texts_list(s, brackets_numbers_list,'')\n",
    "\n",
    "    s = re.sub('[^A-Za-z0-9.]+', ' ', s)  # Remove all punctuations, except from .\n",
    "    s = re.sub(r'\\w*\\d+\\w*', '', s)       # Remove digits\n",
    "\n",
    "    #replace strings\n",
    "    s = s.replace(\"i.e\", \" \")  # remove i.e\n",
    "    s = s.replace(\"run.\", \" \")  # remove run.\n",
    "    s = s.replace(\".zip\", \" \")  # remove .zip\n",
    "    s = s.replace(\"server.xml\", \" \")  # remove server.xml\n",
    "    s = s.replace(\".yml\", \" \")  # remove .yml\n",
    "    s = s.replace(\".pai\", \" \")  # remove .pai\n",
    "    s = s.replace(\".xml\", \" \")  # remove .xml\n",
    "    s = s.replace(\".run\", \" \")  # remove .rum\n",
    "    s = s.replace(\".com\", \" \")  # remove .com\n",
    "\n",
    "    s = re.sub('\\s{2,}', \" \", s)  # Replace the over spaces\n",
    "\n",
    "    text_sentences = split_text_string_into_sentences_v2(s, max_len)\n",
    "    \n",
    "    return text_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# FIND SUB-STRING IN STRING (ready for multiple cases)\n",
    "# -------------------------------------------------------------------------\n",
    "import re\n",
    "\n",
    "def find_entity_matches(sentence, ent):\n",
    "    entity_match_indexes = [m.start() for m in re.finditer(r'{}'.format(re.escape(ent)), sentence)]\n",
    "    return entity_match_indexes\n",
    "\n",
    "# # UNIT TESTING:\n",
    "# sentence = 'branding cortez will be branded as follows cr for install and vs integration package sap crystal reports version for visual studio cr runtime install package sap crystal reports runtime engine for'\n",
    "# text_sentences = ['branding cortez will be branded as follows cr for install and vs integration package sap crystal reports version for visual studio cr runtime install package sap crystal reports runtime engine for']\n",
    "# ent = 'sap crystal reports' \n",
    "# entity_match_indexes = find_entity_matches(sentence, ent)\n",
    "# entity_match_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = '1920507669\tDigital River order flow for \"Crystal Products\" via bobj platform Example Products Lumira Crystal products without maitenance OpenSAP Digital River order flow for \"Subscription Products\" via sapsubs platform Note: For checkouts where user is not charged like for trials, TSC for privileged user and free edition checkout, payment section is hidden (hence no DR iframe in Checkout V2). Example Products DCRM HCP TSC Solution Manager products Digital River order flow for termination Digital River order flow for change quantity (Upsell/Downsell) Note: For upsell orders, quantity change is applicable at the same time. For down-sell orders, quantity change is applicable in the next renewal cycle. 1824109303\tThis page contains high level concept of Fulfillment Hub defining main goals and boundaries of this system. Main goals and responsibilities Main pain points with current architecture: Each back-end system has heavy tightly coupled logic currently to support fulfillment and many cases actions need to be done across few systems. no easy end to end monitoring of order fulfillment, problems are usually discovered when user complains on-boarding of new products usually means development and release cycle in all involved systems what lead to long time to market cycle Goals: Be able to deploy changes in each system independently, deliver new fulfillment processes with more agility than currently, changes of the process should not lead to development and deployment in respective systems User Experience - faster fulfillment to the customer without waiting on CRM/ERP order processing, user should get his access to ordered products in minutes End to End Monitoring - Ability to track down fulfillment problems to see where in the process the fulfillment for a customer order is stuck, proactively monitor and escalate issues easier product on-boarding process Transparency of fulfillment process status to any system consuming this information Responsibilities: execute and monitor order fulfillment and termination process orchestrate actions of all legacy systems together with triggering provisioning/transaction emails provide a tool where admin user will be able to react on process errors provide a tool where business users in predefined circumstances will be able to perform manual actions with fulfillment process - example: not possible process GTS check as there is no GTS code provided, product content team will need to finalize product configuration Out of scope (boundaries) : it is not role of Fulfillment to set up products and product content in all related systems, this should happen as configuration of those products in respective systems, mapping of products should happen on redesign of fulfillment process for existing products, we are focused on new products on-boarding, this could be part of scope for future, decisions will be made per product Fulfillment hub is not a ESB tool and will not have role of integration platform, PI is still in place and will take care about exposing, controlling and securing all existing and future integration, Fulfillment HUB will heavily use PI interfaces It is not the role of fulfillment hub to gather and consolidate list of active subscriptions / services what customer has It is not role of Fulfillment Hub to trigger upgrade or renewal process, fulfillment of such process can be defined in Fulfillment Hub but trigger mechanism belongs to respective system (eCommerce part or back-end system managing the solution/tenant) Fulfillment Hub is not responsible for order processing (payment capture, invoice creation etc), this are responsibilities of legacy systems, In case there is a problem with payment for the order legacy systems should be able to trigger termination processes in Fulfillment hub Solution Comparision (SWOT) Activiti Engine SAP HANA Cloud Platform Integration SAP Operational Process Inteligence SAP PO SAP Workflow Detailed Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_sentences1 = clean_text_string_and_break_into_sentences_v1(txt, 256)\n",
    "# print(len(text_sentences1))\n",
    "# text_sentences1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_sentences2 = clean_text_string_and_break_into_sentences_v2(txt, 256)\n",
    "# print(len(text_sentences2))\n",
    "# text_sentences2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXECUTE FUNCTIONALITY LOOPING OVER THE SOURCE FILES: (v.2)\n",
    "### LOAD CORP WIKI TEXT, CLEAN DATA, ANNOTATE FILE WITH SAP-ENTITIES, SAVE TO LOCAL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(1,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 1) LOAD SAP-PRODUCT_ENTITIES:\n",
    "# -------------------------------------------------------------------------\n",
    "# PRODUCTS_FOLDER = \"sap-products/\"\n",
    "# sap_products_file = \"prod_full_sap-L1_v2.csv\"\n",
    "\n",
    "sap_ent_prod =[]\n",
    "fileObject = open(PRODUCTS_FOLDER+sap_products_file, \"r\")\n",
    "sap_ent_prod = fileObject.read()\n",
    "sap_ent_prod = sap_ent_prod.split('\\n')\n",
    "# sap_ent_prod\n",
    "\n",
    "# Convert to lower case:\n",
    "sap_ent_prod_lwr = []\n",
    "for p in sap_ent_prod:\n",
    "    p = p.replace('\\ufeff','')\n",
    "    sap_ent_prod_lwr.append(p.lower())\n",
    "\n",
    "sap_ent_prod_lwr = sorted(set(sap_ent_prod_lwr))\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# LOAD CORP WIKI TEXT, CLEAN DATA, ANNOTATE FILE WITH SAP-ENTITIES, SAVE TO LOCAL FILE\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# FOLDER = \"corp_wiki_raw_texts/\"   *** MODED TO TOP OF THE NOTEBOOK\n",
    "# max_len = 256\n",
    "# file_number = np.arange(151, 161)\n",
    "\n",
    "for n in file_number:\n",
    "    # -------------------------------------------------------------------------\n",
    "    # LOAD CORP WIKI TEXT\n",
    "    # -------------------------------------------------------------------------\n",
    "    print('inicio ', n)\n",
    "    file_path_from = FOLDER+\"corp-wiki-\"+str(n)\n",
    "    print(file_path_from)\n",
    "    fileObject = open(file_path_from, \"r\")\n",
    "    text_string = fileObject.read()\n",
    "    text_string = text_string.lower()\n",
    "    #text_string\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # CLEAN DATA and BREAK INTO SENTENCES OF SIZE\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    text_sentences = clean_text_string_and_break_into_sentences_v2(text_string, max_len)\n",
    "    # text_sentences\n",
    "\n",
    "    #release memory\n",
    "    text_string = []\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # FIND SAP ENTITIES IN SENTENCES and PRODUCE BILUO ANNOTATED FILE (FINAL)\n",
    "    # -------------------------------------------------------------------------\n",
    "    ent_list = []; counter = 1; annotated_sentence = [];\n",
    "    annotated_text_SAP = []; entity_tag = []; entities_found = []\n",
    "    ent_found_all = []; words_in_sentence=[];sentence_count=[]; sentences_with_counter=[];\n",
    "    sentence_count_item=''; sentence_count_full=[]; words_in_sentence_full=[];\n",
    "    sentence_count_seq=[];words_seq=[]; pos_seq=[];tags_seq=[];\n",
    "    sentences_list=[];\n",
    "\n",
    "    from re import search\n",
    "    \n",
    "#     from spacy.gold import biluo_tags_from_offsets  # ddeprecated version, replaced by offsets_to_biluo_tags (see below)\n",
    "    from spacy.training import offsets_to_biluo_tags\n",
    "\n",
    "\n",
    "    counter = 1\n",
    "\n",
    "    for sentence in text_sentences:     #full-run\n",
    "\n",
    "        doc = nlp(sentence)\n",
    "        #print(doc)\n",
    "        sentence_count_item = \"Sentence: \" + str(counter)\n",
    "\n",
    "        sentence_count = []\n",
    "        entities_found = []\n",
    "        words_in_sentence = []\n",
    "        #loop over prod_entities and verify if they appear on the text\n",
    "        for ent in sap_ent_prod_lwr:    #full-run\n",
    "    #     for ent in prod_ent_lwr:          #unit-testing\n",
    "    \n",
    "            entity_match_indexes = find_entity_matches(sentence, ent)   # CHANGE-1\n",
    "            for i in entity_match_indexes:                              # CHANGE-1\n",
    "                #print('#' + ent +'# found in #'+ sentence)             # CHANGE-1\n",
    "                #print(ent, '- indexes: ', entity_match_indexes)        # CHANGE-1\n",
    "                pos_s = i  #position start                              # CHANGE-1\n",
    "                pos_e = pos_s + len(ent) #position end                  # CHANGE-1\n",
    "                entity_tag = (pos_s, pos_e, \"PROD\")                     # CHANGE-1\n",
    "                entities_found.append(entity_tag)                       # CHANGE-1\n",
    "                \n",
    "                # NOTE: CHANGE-1 IS TO BEING ABLE TO FIND MULTIPLE TIME THE SAME SUBSTRING IN A SENTENCE\n",
    "            \n",
    "#             if search(ent, sentence):                         # previous-version # CHANGE-1\n",
    "#                 #print('#' + ent +'# found in #'+ sentence)   # previous-version # CHANGE-1\n",
    "#                 #print(ent)                                   # previous-version # CHANGE-1\n",
    "#                 pos_s = sentence.index(ent)  #position start  # previous-version # CHANGE-1\n",
    "#                 pos_e = pos_s + len(ent) #position end        # previous-version # CHANGE-1\n",
    "#                 entity_tag = (pos_s, pos_e, \"PROD\")           # previous-version # CHANGE-1\n",
    "#                 entities_found.append(entity_tag)             # previous-version # CHANGE-1\n",
    "#     #             tags = biluo_tags_from_offsets(doc, entity_tag)  # previous-version # CHANGE-1\n",
    "#     #             entities_found.append(tags)                 # previous-version # CHANGE-1\n",
    "\n",
    "        #tags  = biluo_tags_from_offsets(doc, entities_found) # deprecated version, replaced by 0ffsets_to_biluo_tags (see below)\n",
    "        tags = offsets_to_biluo_tags(doc, entities_found)\n",
    "    #     print(entities_found)\n",
    "    #     print(tags)\n",
    "\n",
    "        #ent_found_all.append(tags) # previous-version # CHANGE-2\n",
    "        #for i in tags:             # previous-version # CHANGE-2\n",
    "        #    tags_seq.append(i)     # previous-version # CHANGE-2\n",
    "            \n",
    "        ent_found_all.append(tags)         # CHANGE-2\n",
    "        for i in tags:                     # CHANGE-2\n",
    "            curr_tag = i                   # CHANGE-2\n",
    "            if curr_tag == 'U-PROD':       # CHANGE-2\n",
    "                curr_tag = 'B-PROD'        # CHANGE-2\n",
    "            elif curr_tag == 'L-PROD':     # CHANGE-2\n",
    "                curr_tag = 'I-PROD'        # CHANGE-2\n",
    "                \n",
    "        # NOTE: CHANGE-2 IS CHANGE 'BILOU' TAGS TO 'BIO' TAGGING\n",
    "        \n",
    "            tags_seq.append(curr_tag)\n",
    "\n",
    "        # add each word to another list\n",
    "        for token in doc:\n",
    "            sentence_count.append(sentence_count_item)\n",
    "            sentence_count_seq.append(sentence_count_item)\n",
    "            words_in_sentence.append(token)\n",
    "            words_seq.append(token)\n",
    "            pos_seq.append(token.tag_)\n",
    "    #     print(sentence_count)\n",
    "    #     print(words_in_sentence)\n",
    "    #     sentence_count_full.append(sentence_count)\n",
    "    #     words_in_sentence_full.append(words_in_sentence)\n",
    "    #     print('-'*60)   \n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "    # print(sentence_count_full)   \n",
    "    # print(words_in_sentence_full)\n",
    "    # print(ent_found_all)\n",
    "\n",
    "    # ALL ANNOTATIONS FROM TEXT_STRING (including sentences with Tag = O)\n",
    "    annotated = {\"Sentence\": sentence_count_seq, \"Word\": words_seq, \"POS\": pos_seq, \"Tag\": tags_seq}\n",
    "    annotated_df = pd.DataFrame(annotated)\n",
    "    # annotated_df\n",
    "\n",
    "    # Get All Sentences in a list\n",
    "    sentences_list=[]\n",
    "    counter = 1\n",
    "    for sentence in text_sentences: \n",
    "        sentence_count_item = \"Sentence: \" + str(counter)\n",
    "        counter_sentence = (sentence_count_item, sentence)\n",
    "        sentences_list.append(counter_sentence)\n",
    "        counter = counter + 1\n",
    "    # sentences_list\n",
    "\n",
    "    # Generate SENTENCES_LIST_DF\n",
    "    sentences_list_df = pd.DataFrame(sentences_list, columns=['Sentence', 'Text'])\n",
    "    # sentences_list_df\n",
    "\n",
    "    # Subset of Annotated sentences with Tag != 'O'\n",
    "    with_annotations_df = annotated_df[annotated_df['Tag'] == 'B-PROD']\n",
    "    # with_annotations_df \n",
    "\n",
    "    # Counting the number of sentences with true annotations\n",
    "    #l = len(with_annotations_df.Sentence.unique())\n",
    "    # print(str(l) + ' unique annotated sentences')\n",
    "\n",
    "    # Get 'unique annotated sentences' in list \"annotated_sentences\" (sentences with at least 1 entity found)\n",
    "    annotated_sentences = []\n",
    "    annotated_sentences = with_annotations_df.Sentence.unique()\n",
    "    # annotated_sentences \n",
    "\n",
    "    # Get all full sentences with true annotations in df: \"annotated_sentences_df\"\n",
    "    annotated_sentences_df =  annotated_df[annotated_df['Sentence'].isin(annotated_sentences)]\n",
    "    # annotated_sentences_df \n",
    "\n",
    "    # Get all sentences from 'sentences_list' that have true annotations\n",
    "    sentences_list_annot_df  =  sentences_list_df[sentences_list_df['Sentence'].isin(annotated_sentences)]\n",
    "    # sentences_list_annot_df \n",
    "    \n",
    "    #reset numbering of Sentences in the Annotated Sentences dataframe\n",
    "    annotated_sentences_df_renumbered = reset_annotated_file_sentence_numbering(annotated_sentences_df, n)\n",
    "    sentences_list_annot_df_renumbered = reset_annotated_file_sentence_numbering(sentences_list_annot_df, n)\n",
    "    #annotated_sentences_df_renumbered\n",
    "    #sentences_list_annot_df_renumbered\n",
    "\n",
    "    # SAVE TO LOCAL FILE:\n",
    "    # a) Annotated file:\n",
    "    file_path_to_a = \"corp_wiki_annotated/corp-wiki-\"+str(n)+\"-annotated_sentences.csv\"\n",
    "    annotated_sentences_df_renumbered.to_csv(file_path_to_a, sep=',',index=False)\n",
    "    # annotated_sentences_df.to_csv(\"corp-wiki-1-annotated_sentences.csv\", sep=',',index=False)\n",
    "\n",
    "    # b) Sentences list:\n",
    "    file_path_to_s = \"corp_wiki_annotated/corp-wiki-\"+str(n)+\"-annotated_sentences_list.csv\"\n",
    "    sentences_list_annot_df_renumbered.to_csv(file_path_to_s, sep=',',index=False)\n",
    "    # sentences_list_annot_df.to_csv(\"corp-wiki-1-annotated_sentences_list.csv\", sep=',',index=False)\n",
    "    \n",
    "    print('fin', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence[91:109]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_count_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tags_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence[46:53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP ---- End of code version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = str(doc)\n",
    "# print(frase)\n",
    "print(frase[2290:2296])\n",
    "print(frase[2290:2319])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_list_annot_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #reset numbering of Sentences in the Annotated Sentences dataframe\n",
    "# annotated_sentences_df_renumbered = reset_annotated_file_sentence_numbering(annotated_sentences_df, n)\n",
    "# sentences_list_annot_df_renumbered = reset_annotated_file_sentence_numbering(sentences_list_annot_df, n)\n",
    "# #annotated_sentences_df_renumbered\n",
    "# #sentences_list_annot_df_renumbered\n",
    "\n",
    "# # SAVE TO LOCAL FILE:\n",
    "# # a) Annotated file:\n",
    "# file_path_to_a = \"corp_wiki_annotated/corp-wiki-\"+str(n)+\"-annotated_sentences.csv\"\n",
    "# annotated_sentences_df_renumbered.to_csv(file_path_to_a, sep=',',index=False)\n",
    "# # annotated_sentences_df.to_csv(\"corp-wiki-1-annotated_sentences.csv\", sep=',',index=False)\n",
    "\n",
    "# # b) Sentences list:\n",
    "# file_path_to_s = \"corp_wiki_annotated/corp-wiki-\"+str(n)+\"-annotated_sentences_list.csv\"\n",
    "# sentences_list_annot_df_renumbered.to_csv(file_path_to_s, sep=',',index=False)\n",
    "# # sentences_list_annot_df.to_csv(\"corp-wiki-1-annotated_sentences_list.csv\", sep=',',index=False)\n",
    "\n",
    "# print('fin', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNIT TESTING CODE:\n",
    "\n",
    "# strg = 'defect management proces basic principles defects are tracked in each implementation team s jira backlog a defect can be logged by anyone in advanced analytics in any backlog it is the responsibility of the receiving team to investigate dispatch defects to the appropriate developer it is the responsibility of the person logging the defect to provide all the pertinent details to facilitate the defect s correction defect corrections are tracked not by fix versions nor by epics but rather by target release defect reporting will be based on target release teams are free to use epics to group defects if that helps them then when an issue is found by team a related to functional area managed by team b team a logs defect in team b s backlog when possible team a indicates the build number in which the issue was found as well as the target release in which the team believes they need the correction team b defect pipe reviewers are responsible to monitor the pipe and dispatch the defect accordingly to team b developers team b takes the responsibility to communicate back to team a for any delivery dates target release changes defect pipe follow up will be done in bi weekly quality management calls with qa leads defect backlog pipe managers'\n",
    "# strg = 'defect management proces basic principles defects are tracked in each implementation team'\n",
    "# print(len(strg))\n",
    "\n",
    "# nr_wrds = strg.split()\n",
    "# print(len(nr_wrds))\n",
    "\n",
    "# s1 = 'defect'\n",
    "# s2 = 'management'\n",
    "# s3 = s1 + ' ' + s2\n",
    "# print(s3)\n",
    "\n",
    "# max_len = 80\n",
    "# # if len(nr_wrds) > 5 and len(nr_wrds) > max_len:\n",
    "# #     print('> 5 and < 128')\n",
    "# if len(nr_wrds) > max_len:\n",
    "#     print('strg > max_len')\n",
    "#     s_trunc1 = strg[:max_len]\n",
    "#     s_trunc2 = strg[max_len:]\n",
    "    \n",
    "# print(s_trunc1)\n",
    "# print(s_trunc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNIT TESTING CODE:\n",
    "# def smart_truncate(content, length=100, suffix=''):\n",
    "#     if len(content) <= length:\n",
    "#         return content\n",
    "#     else:\n",
    "#         return ' '.join(content[:length+1].split(' ')[0:-1]) + suffix\n",
    "    \n",
    "# truncated1 = smart_truncate(strg, length=50, suffix='')\n",
    "# print(truncated1)\n",
    "# print(len(truncated1))\n",
    "# index_trunc1 = len(truncated1)\n",
    "# truncated2 = strg[index_trunc1:]\n",
    "# print(truncated2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----------------------------------------------   END OF CODE  ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "============================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CorpWiki article numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"corp_wiki_raw_texts/\"\n",
    "max_len = 256\n",
    "file_number = np.arange(1,2)\n",
    "text_string = ' '\n",
    "\n",
    "for n in file_number:\n",
    "    # -------------------------------------------------------------------------\n",
    "    # LOAD CORP WIKI TEXT\n",
    "    # -------------------------------------------------------------------------\n",
    "    print('inicio ', n)\n",
    "    file_path_from = FOLDER+\"corp-wiki-\"+str(n)\n",
    "    print(file_path_from)\n",
    "    fileObject = open(file_path_from, \"r\")\n",
    "    text_string = fileObject.read()\n",
    "    text_string = text_string.lower()\n",
    "    text_string = text_string + \". End-corp-wiki-\"+str(n)+ '. '\n",
    "    \n",
    "print('End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = text_string.lower()\n",
    "text_string = ''\n",
    "\n",
    "s = re.sub(\"@\\S+\", \" \", s)    #remove twitter account\n",
    "s = re.sub(\"https*\\S+\", \" \", s)  #remove url\n",
    "s = s.replace(\"\\t\", \" \")  # remove tabs\n",
    "# s = s.replace(\"\\n\",\". \")  # remove line-breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = text_string.lower()\n",
    "new_s = s.splitlines()\n",
    "len(new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '2193769388 for development and testing, we have a \"test\" fs-pm system that\\'s setup for us to use for abv and development testing. often times, this is enough, but there are situations where we need to connect to another system (performance testing, load testing). depending on your needs and requirements for the test, contact someone on the fs-pm team to see which system we should use. once a system is identified, we need to do a sanity check to see if this system is usable for pqm. sanity check on new fs-pm system: fs-pm integration user and connection details to the system. availability of business partners integration to commissions (getting help from fs-pm team  contact lars)'\n",
    "\n",
    "scw_ids = []\n",
    "\n",
    "for i in new_s:\n",
    "    scw_ids.append(i[:10])\n",
    "    \n",
    "len(scw_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scw_ids[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_s_text_only = []\n",
    "temp = []\n",
    "\n",
    "for i in new_s:\n",
    "    new_line = i.split(i[:10])\n",
    "    temp.append(new_line)\n",
    "\n",
    "for t in temp:\n",
    "    for i in t:\n",
    "        if i != '':\n",
    "            i = i.strip() \n",
    "            new_s_text_only.append(i)\n",
    "        \n",
    "len(new_s_text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_s_text_only[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_s_text_only_20 = []\n",
    "for i in new_s_text_only:\n",
    "    new_s_text_only_20.append(i[:20])\n",
    "    \n",
    "new_s_text_only_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE CLEANSING FUNCTIONS (v.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE CLEANSING FUNCTIONS:\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# function to replace spscific SAP-product strings in text\n",
    "# -------------------------------------------------------------------------\n",
    "def replace_strings_in_text(text_string, replacement_list):\n",
    "    for i in replacement_list:\n",
    "        new_text = text_string.replace(i[0], i[1])\n",
    "        text_string = new_text; new_text = ''\n",
    "    return text_string\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# create function to remove numbers in brackets from text_string\n",
    "# -------------------------------------------------------------------------\n",
    "def generate_numbers_in_brackets_list(n=200):\n",
    "    numbers_list = np.arange(1,n)\n",
    "    brackets_numbers_list = []\n",
    "    for i in numbers_list:\n",
    "        string = '[' + str(i) + ']'\n",
    "        brackets_numbers_list.append(string)\n",
    "    return brackets_numbers_list\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# create function to replace a string with another string\n",
    "# -------------------------------------------------------------------------\n",
    "def replace_string_from_texts_list(string, strings_list, replace_by):\n",
    "    for s in strings_list:\n",
    "        new_string = string.replace(s, replace_by)\n",
    "    return new_string\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "#CLEAN STRING_TEXT (remove special chacters, line-breaks, hashtags, urls, numbers, \n",
    "# over spaces,leading/trailing spaces, punctuations\n",
    "# -------------------------------------------------------------------------\n",
    "def clean_text_chars(string_text):\n",
    "    x = string_text\n",
    "#     y = x.lower()\n",
    "#     y = re.sub(\"@\\S+\", \" \", y)    #remove twitter account\n",
    "#     y = re.sub(\"https*\\S+\", \" \", y)  #remove url\n",
    "#     y = y.replace(\"\\t\", \" \")  # remove tabs\n",
    "#     y = y.replace(\"\\n\",\". \")  # remove line-breaks\n",
    "#     y = re.sub(\"#\\S+\", \" \", y) # Remove hashtags\n",
    "#     y = re.sub(\"\\'\\w+\", '', y) # Remove ticks and the next character\n",
    "#     #y = re.sub('[%s]' % re.escape(y.punctuation), ' ', y) # Remove punctuations\n",
    "#     #y = re.sub('\\W+',' ', y)  # Remove punctuations\n",
    "#     y = re.sub('[^A-Za-z0-9.]+', ' ', y)  # Remove all punctuations, except from .\n",
    "#     #y = re.sub(r'\\w*\\d+\\w*', '', y) # Remove numbers\n",
    "#     y = re.sub(r'\\w*\\d+\\w*', '', y)\n",
    "#     y = y.strip()\n",
    "#     #     y = y.replace('  ', ' ')\n",
    "#     y = re.sub('\\s{2,}', \" \", y)  # Replace the over spaces\n",
    "#     y =  re.sub(r\"\\b[a-zA-Z]\\b\", \"\", y)\n",
    "#     y = re.sub(r'(?:^| )\\w(?:$| )', ' ', y).strip()\n",
    "\n",
    "    return y\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# complete data-cleaning of text (includes: \"replace_string_from_texts_list\" and \"clean_text_chars\"functions)\n",
    "# -------------------------------------------------------------------------\n",
    "def clean_text_complete(input_text):\n",
    "    #generate list of numbers in brackets (this normally appear in most wiki)\n",
    "    brackets_numbers_list = generate_numbers_in_brackets_list(n=200)\n",
    "    \n",
    "    #remove those numbers in brackes from the text\n",
    "    text_string2 = replace_string_from_texts_list(text_string, brackets_numbers_list,'')\n",
    "    \n",
    "    #clean text chars\n",
    "    cleaned_text_string = clean_text_chars(text_string2)\n",
    "    \n",
    "    return cleaned_text_string\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# SPLIT DATA_STRING INTO SENTENCES\n",
    "# -------------------------------------------------------------------------\n",
    "def split_text_string_into_sentences(text_string):\n",
    "    text_sentences = []\n",
    "    text_sentences_tmp = text_string.split('.')\n",
    "    for s in text_sentences_tmp:\n",
    "        if len(s) > 5:\n",
    "            s_nlts = s.strip()\n",
    "            s_nlts = s_nlts.replace('     ',' ') # replace 5 spaces by 1 space\n",
    "            s_nlts = s_nlts.replace('    ',' ') # replace 4 spaces by 1 space\n",
    "            s_nlts = s_nlts.replace('   ',' ') # replace 3 spaces by 1 space\n",
    "            s_nlts = s_nlts.replace('  ',' ') # replace 2 spaces by 1 space\n",
    "            text_sentences.append(s_nlts)\n",
    "\n",
    "    return text_sentences\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# RESET SENTENCE NUMBERING OF ANNOTATED SENTENCES DATAFRAME\n",
    "# -------------------------------------------------------------------------\n",
    "def reset_annotated_file_sentence_numbering(df_data, n):\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "    previous_sentence = ''\n",
    "    counter = 1\n",
    "    sentence_counter = 'Sentence: '+str(n)+'_'+str(counter)\n",
    "\n",
    "    for i in df_data.index: \n",
    "        if i == 0:\n",
    "            previous_sentence = df_data['Sentence'][i]\n",
    "            df_data['Sentence'][i] = 'Sentence: '+str(n)+'_1'\n",
    "\n",
    "        else:\n",
    "            if df_data['Sentence'][i] == previous_sentence:\n",
    "                previous_sentence = df_data['Sentence'][i]\n",
    "                df_data['Sentence'][i] = sentence_counter\n",
    "            else:\n",
    "                counter = counter + 1\n",
    "                sentence_counter = 'Sentence: '+str(n)+'_'+str(counter)\n",
    "                previous_sentence = df_data['Sentence'][i]\n",
    "                df_data['Sentence'][i] = sentence_counter\n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXECUTE FUNCTIONALITY LOOPING OVER THE SOURCE FILES: (v.1)\n",
    "### LOAD CORP WIKI TEXT, CLEAN DATA, ANNOTATE FILE WITH SAP-ENTITIES, SAVE TO LOCAL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# LOAD CORP WIKI TEXT\n",
    "# -------------------------------------------------------------------------\n",
    "file_number = np.arange(101,111)\n",
    "for n in file_number:\n",
    "    print('inicio ', n)\n",
    "    file_path_from = \"corp_wiki_texts/corp-wiki-\"+str(n)\n",
    "    print(file_path_from)\n",
    "    fileObject = open(file_path_from, \"r\")\n",
    "    text_string = fileObject.read()\n",
    "    text_string = text_string.lower()\n",
    "    #text_string\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # CLEAN DATA\n",
    "    # -------------------------------------------------------------------------\n",
    "    cleaned_text_string = clean_text_complete(text_string)\n",
    "    # cleaned_text_string\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # EXECUTE SAP-PRODUCTS RELATED STRING REPLACEMENTS:\n",
    "    # -------------------------------------------------------------------------\n",
    "    cleaned_text_string = replace_strings_in_text(cleaned_text_string,replace)\n",
    "    # cleaned_text_string\n",
    "\n",
    "    #release memory\n",
    "    text_string = []\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # SPLIT DATA_STRING INTO SENTENCES\n",
    "    # -------------------------------------------------------------------------\n",
    "    text_sentences = split_text_string_into_sentences(cleaned_text_string)\n",
    "    # text_sentences\n",
    "\n",
    "    #release memory\n",
    "    cleaned_text_string = []\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # FIND SAP ENTITIES IN SENTENCES and PRODUCE BILUO ANNOTATED FILE (FINAL)\n",
    "    # -------------------------------------------------------------------------\n",
    "    ent_list = []; counter = 1; annotated_sentence = [];\n",
    "    annotated_text_SAP = []; entity_tag = []; entities_found = []\n",
    "    ent_found_all = []; words_in_sentence=[];sentence_count=[]; sentences_with_counter=[];\n",
    "    sentence_count_item=''; sentence_count_full=[]; words_in_sentence_full=[];\n",
    "    sentence_count_seq=[];words_seq=[]; pos_seq=[];tags_seq=[];\n",
    "    sentences_list=[];\n",
    "\n",
    "    from re import search\n",
    "    #from spacy.gold import biluo_tags_from_offsets# deprecated version, replaced by 0ffsets_to_biluo_tags (see below)\n",
    "    from spacy.training import offsets_to_biluo_tags\n",
    "\n",
    "    counter = 1\n",
    "\n",
    "    for sentence in text_sentences:     #full-run\n",
    "    # for sentence in text_sentences_lwr:   #unit-testing\n",
    "        sentence = sentence.replace('sap_hanalite', '')\n",
    "        sentence = sentence.replace('sap hana_xs', 'sap hana_xsa')\n",
    "        doc = nlp(sentence)\n",
    "        print(doc)\n",
    "        sentence_count_item = \"Sentence: \" + str(counter)\n",
    "\n",
    "        sentence_count = []\n",
    "        entities_found = []\n",
    "        words_in_sentence = []\n",
    "        #loop over prod_entities and verify if they appear on the text\n",
    "        for ent in sap_ent_prod_lwr:    #full-run\n",
    "    #     for ent in prod_ent_lwr:          #unit-testing\n",
    "            if search(ent, sentence):\n",
    "                #print('#' + ent +'# found in #'+ sentence)\n",
    "                #print(ent)\n",
    "                pos_s = sentence.index(ent)  #position start\n",
    "                pos_e = pos_s + len(ent) #position end\n",
    "                entity_tag = (pos_s, pos_e, \"PROD\")\n",
    "                entities_found.append(entity_tag)\n",
    "    #             tags = biluo_tags_from_offsets(doc, entity_tag)\n",
    "    #             entities_found.append(tags)\n",
    "\n",
    "        #tags  = biluo_tags_from_offsets(doc, entities_found)  # deprecated version, replaced by 0ffsets_to_biluo_tags (see below)\n",
    "        tags = offsets_to_biluo_tags(doc, entities_found)\n",
    "    #     print(entities_found)\n",
    "    #     print(tags)\n",
    "\n",
    "        ent_found_all.append(tags)\n",
    "        for i in tags:\n",
    "            tags_seq.append(i)\n",
    "\n",
    "        # add each word to another list\n",
    "        for token in doc:\n",
    "            sentence_count.append(sentence_count_item)\n",
    "            sentence_count_seq.append(sentence_count_item)\n",
    "            words_in_sentence.append(token)\n",
    "            words_seq.append(token)\n",
    "            pos_seq.append(token.tag_)\n",
    "    #     print(sentence_count)\n",
    "    #     print(words_in_sentence)\n",
    "    #     sentence_count_full.append(sentence_count)\n",
    "    #     words_in_sentence_full.append(words_in_sentence)\n",
    "    #     print('-'*60)   \n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "    # print(sentence_count_full)   \n",
    "    # print(words_in_sentence_full)\n",
    "    # print(ent_found_all)\n",
    "\n",
    "    # ALL ANNOTATIONS FROM TEXT_STRING (including sentences with Tag = O)\n",
    "    annotated = {\"Sentence\": sentence_count_seq, \"Word\": words_seq, \"POS\": pos_seq, \"Tag\": tags_seq}\n",
    "    annotated_df = pd.DataFrame(annotated)\n",
    "    # annotated_df\n",
    "\n",
    "    # Get All Sentences in a list\n",
    "    sentences_list=[]\n",
    "    counter = 1\n",
    "    for sentence in text_sentences: \n",
    "        sentence_count_item = \"Sentence: \" + str(counter)\n",
    "        counter_sentence = (sentence_count_item, sentence)\n",
    "        sentences_list.append(counter_sentence)\n",
    "        counter = counter + 1\n",
    "    # sentences_list\n",
    "\n",
    "    # Generate SENTENCES_LIST_DF\n",
    "    sentences_list_df = pd.DataFrame(sentences_list, columns=['Sentence', 'Text'])\n",
    "    # sentences_list_df\n",
    "\n",
    "    # Subset of Annotated sentences with Tag != 'O'\n",
    "    with_annotations_df = annotated_df[annotated_df['Tag'] != 'O']\n",
    "    # with_annotations_df \n",
    "\n",
    "    # Counting the number of sentences with true annotations\n",
    "    l = len(with_annotations_df.Sentence.unique())\n",
    "    # print(str(l) + ' unique annotated sentences')\n",
    "\n",
    "    # Get 'unique annotated sentences' in list \"annotated_sentences\" (sentences with at least 1 entity found)\n",
    "    annotated_sentences = []\n",
    "    annotated_sentences = with_annotations_df.Sentence.unique()\n",
    "    # annotated_sentences \n",
    "\n",
    "    # Get all full sentences with true annotations in df: \"annotated_sentences_df\"\n",
    "    annotated_sentences_df =  annotated_df[annotated_df['Sentence'].isin(annotated_sentences)]\n",
    "    # annotated_sentences_df \n",
    "\n",
    "    # Get all sentences from 'sentences_list' that have true annotations\n",
    "    sentences_list_annot_df  =  sentences_list_df[sentences_list_df['Sentence'].isin(annotated_sentences)]\n",
    "    # sentences_list_annot_df \n",
    "    \n",
    "    #reset numbering of Sentences in the Annotated Sentences dataframe\n",
    "    annotated_sentences_df_renumbered = reset_annotated_file_sentence_numbering(annotated_sentences_df, n)\n",
    "    sentences_list_annot_df_renumbered = reset_annotated_file_sentence_numbering(sentences_list_annot_df, n)\n",
    "    #annotated_sentences_df_renumbered\n",
    "    #sentences_list_annot_df_renumbered\n",
    "\n",
    "    # SAVE TO LOCAL FILE:\n",
    "    # a) Annotated file:\n",
    "    file_path_to_a = \"corp_wiki_annotated/corp-wiki-\"+str(n)+\"-annotated_sentences.csv\"\n",
    "    annotated_sentences_df_renumbered.to_csv(file_path_to_a, sep=',',index=False)\n",
    "    # annotated_sentences_df.to_csv(\"corp-wiki-1-annotated_sentences.csv\", sep=',',index=False)\n",
    "\n",
    "    # b) Sentences list:\n",
    "    file_path_to_s = \"corp_wiki_annotated/corp-wiki-\"+str(n)+\"-annotated_sentences_list.csv\"\n",
    "    sentences_list_annot_df_renumbered.to_csv(file_path_to_s, sep=',',index=False)\n",
    "    # sentences_list_annot_df.to_csv(\"corp-wiki-1-annotated_sentences_list.csv\", sep=',',index=False)\n",
    "    \n",
    "    print('fin', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NAMED ENTITY RECOGNITION with SPACY:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the pre-trained entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the Named Entity Reconition with SpaCy\n",
    "# `doc.ents()` function returns the entities from a given text stored in doc variable\n",
    "# `doc.ents.text` and `doc.ents.label_` return the name of the entities and label or entity type respectively\n",
    "\n",
    "#SAMPLE: \n",
    "\n",
    "#Calling the nlp object on a string of text will return a processed Doc:\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print('sample sentence:  ', doc)\n",
    "\n",
    "print('-'*50)\n",
    "print('NER (Named Entity Recognition)')\n",
    "print('-'*50)\n",
    "# Get Entities Information\n",
    "print([(X.text, X.label_) for X in doc.ents]) \n",
    "\n",
    "# using the Spacy Named Entity Recognition visualizer\n",
    "#spacy.displacy.serve(doc, style='ent')\n",
    "\n",
    "#POS (Part-of-Speech tags)\n",
    "print('-'*50)\n",
    "print('#POS (Part-of-Speech tags)')\n",
    "print('-'*50)\n",
    "for token in doc:\n",
    "#     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "#             token.shape_, token.is_alpha, token.is_stop)\n",
    "    print(token.text, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to given text and produce annotated file with pre-trained Standard Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ent_list = []\n",
    "# counter = 1\n",
    "# annotated_sentence = []\n",
    "# annotated_text = []\n",
    "\n",
    "# for s in text_sentences:\n",
    "#     doc = nlp(s)\n",
    "#     ent_list = [(X.text, X.label_) for X in doc.ents]\n",
    "#     #example: ('Thousands', 'CARDINAL'), ('London', 'GPE'), ('Iraq', 'GPE'), ('British', 'NORP')]\n",
    "#     for token in doc:\n",
    "#         tag = ''\n",
    "#         annotated_sentence.append('Sentence: '+str(counter))   #column: Sentence #\n",
    "# #         annotated_sentence.append(s)\n",
    "#         annotated_sentence.append(token.text)                  #column: Word           \n",
    "#         annotated_sentence.append(token.tag_)                  # column: POS\n",
    "#         word = str(token)\n",
    "# #         print('-'*10)\n",
    "# #         print(word)\n",
    "#         for i in ent_list:\n",
    "# #             print(token)\n",
    "# #             print(i[0])\n",
    "# #             print(i[1])\n",
    "# #         print('-'*10)\n",
    "            \n",
    "#             ent_w = str(i[0])\n",
    "#             ent_t = str(i[1])\n",
    "# #             print(entity)\n",
    "#             if word == ent_w :\n",
    "# #                 print('word equals entity')\n",
    "#                 tag = ent_t\n",
    "\n",
    "#         if tag == '':\n",
    "#             tag = 'O'\n",
    "        \n",
    "#         annotated_sentence.append(tag)                          #column: Tag\n",
    "\n",
    "#     annotated_text.append(annotated_sentence)\n",
    "    \n",
    "# annotated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE Standard Entities Annotated list to Local File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(\"corp_wiki_texts/corp-wiki-1-annotated.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(annotated_text, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #release memory\n",
    "# annotated_text = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD Standard Entities Annotated list to Local File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"corp_wiki_texts/corp-wiki-1-annotated.txt\", \"rb\") as data:\n",
    "#     W = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear table\n",
    "#W = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data = pd.DataFrame(W, columns=['Sentence #','Word','POS','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and Annotate SAP Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Example for building the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SMALL DATA SAMPLES FOR UNIT TESTING:\n",
    "\n",
    "# prod_entities = ['SAP Cloud Foundry','SAP Conversational AI','SAP S/4HANA','SAP Cloud Platform','SAP Gateway','SAP HANA']\n",
    "# text_sentences = ['SAP S/4HANA runs on SAP HANA database and can be extended on SAP Cloud Foundry','SAP Conversational AI can be integrated with SAP S/4HANA via SAP Cloud Foundry']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to lower case:\n",
    "# prod_ent_sample_lwr = []\n",
    "# for p in prod_entities_sample:\n",
    "#     prod_ent_sample_lwr.append(p.lower())\n",
    "    \n",
    "# print(prod_ent_sample_lwr)\n",
    "\n",
    "# text_sentences_lwr = []\n",
    "# for s in text_sentences_sample:\n",
    "#     text_sentences_sample_lwr.append(s.lower())\n",
    "    \n",
    "# print(text_sentences_sample_lwr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIT TEST: FIND SAP ENTITIES IN SENTENCES and PRODUCE BILUO TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIND SAP ENTITIES IN SENTENCES and PRODUCE BILUO TAGS\n",
    "# ent_list = []; counter = 1; annotated_sentence = [];\n",
    "# annotated_text_SAP = []; entity_tag = []; entities_found = []\n",
    "# ent_found_all = []\n",
    "\n",
    "# from re import search\n",
    "# from spacy.gold import biluo_tags_from_offsets\n",
    "\n",
    "# for sentence in text_sentences_lwr:\n",
    "#     doc = nlp(sentence)\n",
    "#     print(doc)\n",
    "#     entities_found = []\n",
    "#     #loop over prod_entities and verify if they appear on the text\n",
    "#     for ent in prod_ent_lwr:\n",
    "#         if search(ent, sentence):\n",
    "#             #print('#' + ent +'# found in #'+ sentence)\n",
    "#             print(ent)\n",
    "#             pos_s = sentence.index(ent)  #position start\n",
    "#             pos_e = pos_s + len(ent) #position end\n",
    "#             entity_tag = (pos_s, pos_e, \"PROD\")\n",
    "#             entities_found.append(entity_tag)\n",
    "# #             tags = biluo_tags_from_offsets(doc, entity_tag)  # deprecated version, replaced by 0ffsets_to_biluo_tags (see below)\n",
    "              #tags = offsets_to_biluo_tags(doc, entities_found)\n",
    "# #             entities_found.append(tags)\n",
    "    \n",
    "#     tags  = biluo_tags_from_offsets(doc, entities_found)\n",
    "#     print(entities_found)\n",
    "#     print(tags)\n",
    "#     print('-'*60)\n",
    "#     ent_found_all.append(entities_found)\n",
    "    \n",
    "# print(ent_found_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUT FROM ABOVE CELL:\n",
    "\n",
    "# sap s/4hana runs on sap hana database and can be extended on sap cloud foundry\n",
    "# sap cloud foundry\n",
    "# sap s/4hana\n",
    "# sap hana\n",
    "# [(61, 78, 'PROD'), (0, 11, 'PROD'), (20, 28, 'PROD')]\n",
    "# ['B-PROD', 'L-PROD', 'O', 'O', 'B-PROD', 'L-PROD', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PROD', 'I-PROD', 'L-PROD']\n",
    "# ------------------------------------------------------------\n",
    "# sap conversational ai can be integrated with sap s/4hana via sap cloud foundry\n",
    "# sap cloud foundry\n",
    "# sap conversational ai\n",
    "# sap s/4hana\n",
    "# [(61, 78, 'PROD'), (0, 21, 'PROD'), (45, 56, 'PROD')]\n",
    "# ['B-PROD', 'I-PROD', 'L-PROD', 'O', 'O', 'O', 'O', 'B-PROD', 'L-PROD', 'O', 'B-PROD', 'I-PROD', 'L-PROD']\n",
    "# ------------------------------------------------------------\n",
    "# [[(61, 78, 'PROD'), (0, 11, 'PROD'), (20, 28, 'PROD')], [(61, 78, 'PROD'), (0, 21, 'PROD'), (45, 56, 'PROD')]]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.4 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.4-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
