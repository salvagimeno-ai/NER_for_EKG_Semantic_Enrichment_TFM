{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD saved model & INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads and inferences a trained sapxBERT_NER model with the sentences from a selected annotated dataset.\n",
    "\n",
    "input parameters:\n",
    "- last_saved_model => the checkpoint file of a saved model to be inferenced\n",
    "- filename => a BIO annotated dataset\n",
    "\n",
    "output parameters:\n",
    "- .csv file (models_inference_true_pred/\"+model_name+filename_main+\"_true_pred.csv)\n",
    "- fields: true_token, true_label, pred_token, pred_label, sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) EXECUTION PARAMETERS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Run to see all models available in the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL MODELS SAVED:\n",
    "MODELS_FOLDER = 'models_saved/'\n",
    "print('All saved models: ')\n",
    "!ls \"{MODELS_FOLDER}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Execution parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXECUTION PARAMETERS:\n",
    "\n",
    "last_saved_model = '2021-03-22_01-22-44.checkpoint' #=2021-03-18_02-35-03 retrained \n",
    "# last_saved_model = '2021-03-21_14-02-50.checkpoint'\n",
    "\n",
    "# filename = 'scw_1-149_220-272_da.csv'\n",
    "# filename = 'scw_01-23_sa_v6.csv'\n",
    "# filename = 'scw_220_272_da.csv'\n",
    "filename = 'nhve_scw_220_272_da.csv'\n",
    "\n",
    "filename_main = filename[:12]\n",
    "filename, filename_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) LOAD Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('installing transformers')\n",
    "!pip install --upgrade transformers==4.2.2 --quiet\n",
    "!pip install transformers[sentencepiece] --quiet #transformers v4.x --> --quiet\n",
    "\n",
    "print('importing packages')\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, BertForTokenClassification, BertConfig\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    #num_labels=len(tag2idx),\n",
    "    num_labels= 7,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_saved_model():\n",
    "\n",
    "    import os.path\n",
    "    import glob\n",
    "    import datetime\n",
    "\n",
    "    path = 'models_saved/'\n",
    "    list_of_files = glob.glob('models_saved/2021*')\n",
    "    list_of_files\n",
    "\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    _, filename = os.path.split(latest_file)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPUs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPUs \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)  \n",
    "    \n",
    "# Pass the model parameters to the GPU.\n",
    "model.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of models available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST MODEL SAVED:  ** COMMENTED BECAUSE WE ARE GETTING THE VALUE FROM THE BEGINNING **\n",
    "# last_saved_model = get_latest_saved_model()\n",
    "# last_saved_model = last_saved_model[:19]+'.checkpoint'\n",
    "# print('Last saved model: ')\n",
    "# print(last_saved_model)\n",
    "\n",
    "# # ALL MODELS SAVED:\n",
    "# MODELS_FOLDER = 'models_saved/'\n",
    "# print('All saved models: ')\n",
    "# !ls \"{MODELS_FOLDER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_model = '2021-03-11_23-52-31'\n",
    "selected_model = last_saved_model\n",
    "PATH = MODELS_FOLDER+selected_model\n",
    "\n",
    "# PATH = MODELS_FOLDER+selected_model+'/'+selected_model+'.checkpoint'\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# # set model status\n",
    "model.eval()\n",
    "# # - or -\n",
    "# #model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MOdel Parameters\n",
    "param_path = PATH[:32]+'_parameters'\n",
    "file_param = param_path[13:]\n",
    "\n",
    "folder = param_path[:13]\n",
    "\n",
    "model_name = PATH[:32]\n",
    "model_name = model_name[13:]\n",
    "\n",
    "folder, model_name, file_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(param_path, 'r') as reader:\n",
    "#     parameters = reader.read()\n",
    "    \n",
    "# valid_source = parameters[241:243]\n",
    "# valid_filenm = parameters[174:183]\n",
    "# valid_source, valid_filenm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = parameters[76:95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD tag2idx, idx2tag, tag2name from file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD tag2idx, idx2tag, tag2name from file: \n",
    "def tag_values_tag2idx_idx2tag_tag2name_from_model(model):\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    # tag2idx\n",
    "    file = open(MODELS_FOLDER+selected_model[:19]+'_'+'tag2idx', 'rb')\n",
    "    tag2idx = pickle.load(file)\n",
    "    #print(tag2idx)\n",
    "\n",
    "    # idx2tag\n",
    "    file = open(MODELS_FOLDER+selected_model[:19]+'_'+'idx2tag', 'rb')\n",
    "    idx2tag = pickle.load(file)\n",
    "    #print(idx2tag)\n",
    "\n",
    "    # tag2name\n",
    "    file = open(MODELS_FOLDER+selected_model[:19]+'_'+'tag2name', 'rb')\n",
    "    tag2name = pickle.load(file)\n",
    "    #print(tag2name)\n",
    "    \n",
    "    tag_values = []\n",
    "\n",
    "\n",
    "    for key in tag2idx.keys():\n",
    "        tag_values.append(key)\n",
    "        \n",
    "    tags_vals = tag_values\n",
    "    \n",
    "    return tags_vals, tag_values, tag2idx, idx2tag, tag2name\n",
    "\n",
    "\n",
    "\n",
    "tags_vals, tag_values, tag2idx, idx2tag, tag2name = tag_values_tag2idx_idx2tag_tag2name_from_model(selected_model)\n",
    "print('tags_vals: ', tags_vals)\n",
    "print('tag_values: ', tag_values)\n",
    "print('tag2idx: ', tag2idx)\n",
    "print('idx2tag: ', idx2tag)\n",
    "print('tag2name: ', tag2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_values = []\n",
    "\n",
    "\n",
    "# for key in tag2idx.keys():\n",
    "#     tag_values.append(key)\n",
    "\n",
    "# tag_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Inference functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_sap_bert(test_sentence, inference_model):\n",
    "    \n",
    "    model = inference_model\n",
    "    \n",
    "    test_sentence = test_sentence.lower()\n",
    "\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "    tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "    #print('tokenized_sentence: ', tokenized_sentence)\n",
    "    \n",
    "    input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    #print('label_indices: ', label_indices)\n",
    "\n",
    "    # join bpe split tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "    #print('tokens: ', tokens)\n",
    "    \n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, label_indices[0]):\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            #new_labels.append(tag_values[label_idx])\n",
    "            new_labels.append(tag2name[label_idx])\n",
    "            new_tokens.append(token)\n",
    "\n",
    "#     for token, label in zip(new_tokens, new_labels):\n",
    "#         print(\"{}\\t{}\".format(label, token))\n",
    "        \n",
    "    prediction = {\"Token\": new_tokens, \"Label\": new_labels}\n",
    "    df = pd.DataFrame(prediction)\n",
    "    \n",
    "    df2 = df[df['Token'] != '[CLS]']   \n",
    "    prediction_df = df2[df2['Token'] != '[SEP]'] \n",
    "        \n",
    "    return prediction_df\n",
    "\n",
    "def inference_sap_bert_to_list(test_sentence, inference_model):\n",
    "    \n",
    "    prediction_df = inference_sap_bert(test_sentence, inference_model)\n",
    "    prediction_list = prediction_df.values.tolist()\n",
    "\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit test Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference model\n",
    "# test_sentence = 'you can get a complete overview of all applications delivered with mss wda in the sap library for sap erp on sap help portal at sap erp enhancement packages erp central component shared services manager self service manager self service wda applications'\n",
    "\n",
    "# test_sentence = 'common object layer brim billing and revenue innovation management ccm cross catalog mapping sap cc sap convergent charging system sap ci sap convergent invoicing smt subscriber mapping table srt subscriber range table odi order distribution'\n",
    "# test_sentence = 'this article is related to hana'\n",
    "test_sentence = 'sap erp is the best erp'\n",
    "\n",
    "inference_sap_bert(test_sentence, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt Labels if necesary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original values:\n",
    "# tags_vals:  ['I-PROD', 'B-PROD', 'O', 'X', '[CLS]', '[SEP]', 'PAD']\n",
    "# tag_values:  ['I-PROD', 'B-PROD', 'O', 'X', '[CLS]', '[SEP]', 'PAD']\n",
    "# tag2idx:  {'I-PROD': 0, 'B-PROD': 1, 'O': 2, 'X': 3, '[CLS]': 4, '[SEP]': 5, 'PAD': 6}\n",
    "# idx2tag:  {0: 'I-PROD', 1: 'B-PROD', 2: 'O', 3: 'X', 4: '[CLS]', 5: '[SEP]', 6: 'PAD'}\n",
    "# tag2name:  {0: 'I-PROD', 1: 'B-PROD', 2: 'O', 3: 'X', 4: '[CLS]', 5: '[SEP]', 6: 'PAD'}\n",
    "\n",
    "# # adapted:\n",
    "# tags_vals =  ['B-PROD', 'I-PROD', 'O', 'X', '[CLS]', '[SEP]', 'PAD']\n",
    "# tag_values =  ['B-PROD', 'I-PROD', 'O', 'X', '[CLS]', '[SEP]', 'PAD']\n",
    "# tag2idx =  {'B-PROD': 0, 'I-PROD': 1, 'O': 2, 'X': 3, '[CLS]': 4, '[SEP]': 5, 'PAD': 6}\n",
    "# idx2tag =   {0: 'B-PROD', 1: 'I-PROD', 2: 'O', 3: 'X', 4: '[CLS]', 5: '[SEP]', 6: 'PAD'}\n",
    "# tag2name  =   {0: 'B-PROD', 1: 'I-PROD', 2: 'O', 3: 'X', 4: '[CLS]', 5: '[SEP]', 6: 'PAD'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Inference with probability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_prob(input_sentence):\n",
    "\n",
    "    import torch.nn as nn\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    #y = softmax(x)\n",
    "\n",
    "    # test_sentence = 'sap erp is the best erp'\n",
    "    # test_sentence = 'common object layer brim billing and revenue innovation management ccm cross catalog mapping sap cc sap convergent charging system sap ci sap convergent invoicing smt subscriber mapping table srt subscriber range table odi order distribution'\n",
    "\n",
    "    # test_sentence = test_sentence.lower()\n",
    "    input_sentence = input_sentence.lower()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "    tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "    #print('tokenized_sentence: ', tokenized_sentence)\n",
    "\n",
    "    input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "    #print('input_ids: ', input_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    #print(output)\n",
    "\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    #print('label_indices: ', label_indices)\n",
    "\n",
    "    #print('tag_values: ', tag_values)\n",
    "    output[0]\n",
    "\n",
    "    np.argmax(output[0].to('cpu').numpy())\n",
    "\n",
    "    import torch.nn as nn\n",
    "    # print(tag_values)\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    y = softmax(output[0])\n",
    "    # print(y)\n",
    "\n",
    "    y.shape\n",
    "\n",
    "    yarr3d = y.to('cpu').numpy()\n",
    "\n",
    "    yarr3d.shape\n",
    "\n",
    "    yarr2d = np.reshape(yarr3d, (y.shape[1], 7))\n",
    "    #yarr2d\n",
    "\n",
    "    yarr2d.shape\n",
    "\n",
    "    label_prob = []\n",
    "    prob = []\n",
    "\n",
    "    for i in yarr2d:\n",
    "        line = []\n",
    "        lbl_ind = np.argmax(i)\n",
    "        #print(i[lbl_idx])\n",
    "        #print(lbl_idx)\n",
    "        line.append(lbl_ind)\n",
    "        line.append(i[lbl_ind])\n",
    "        prob.append(i[lbl_ind])\n",
    "        label_prob.append(line)\n",
    "\n",
    "    # print(label_prob)\n",
    "    # print(prob)\n",
    "\n",
    "    # for i in label_indices[0]:\n",
    "    #     print(tag_values[i])\n",
    "    \n",
    "    # join the split tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "    #print('tokens: ', tokens)\n",
    "\n",
    "    # new code version (taking the probability into account)\n",
    "    new_tokens, new_labels, new_probs = [], [], []\n",
    "    tkn_lbl_prob = []\n",
    "\n",
    "    for token_, label_, prob_ in zip(tokens, label_indices[0], prob):\n",
    "        line = []\n",
    "        line = (token_, label_, prob_)\n",
    "        tkn_lbl_prob.append(line)\n",
    "\n",
    "    for token_, label_, prob_ in tkn_lbl_prob:\n",
    "        if token_.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token_[2:]\n",
    "        else:\n",
    "            new_labels.append(tag_values[label_])\n",
    "            new_tokens.append(token_)\n",
    "            new_probs.append(prob_)\n",
    "\n",
    "    pred_prob = {\"Token\": new_tokens, \"Label\": new_labels, \"Probability\": new_probs}\n",
    "    df = pd.DataFrame(pred_prob)\n",
    "    df\n",
    "\n",
    "    df2 = df[df['Token'] != '[CLS]']   \n",
    "    prediction_df = df2[df2['Token'] != '[SEP]'] \n",
    "    return prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sentence = 'you can get a complete overview of all applications delivered with mss wda in the sap library for sap erp on sap help portal at sap erp enhancement packages erp central component shared services manager self service manager self service wda applications'\n",
    "test_sentence = 'common object layer brim billing and revenue innovation management ccm cross catalog mapping sap cc sap convergent charging system sap ci sap convergent invoicing smt subscriber mapping table srt subscriber range table odi order distribution'\n",
    "#test_sentence = 'this article is related to hana'\n",
    "# test_sentence = 'sap erp is the best erp'\n",
    "\n",
    "inference_with_prob(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) INFERENCE: Apply model to a TRAINING SET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('installing tensorflow')\n",
    "!pip install tensorflow --quiet\n",
    "print('installing keras')\n",
    "!pip install keras --quiet\n",
    "print('importing packages')\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_values_tag2idx_idx2tag_tag2name_from_data(data):\n",
    "\n",
    "    tags_vals = list(set(data[\"Tag\"].values))\n",
    "\n",
    "    # Add some additional tags:\n",
    "    # X  tag for word piece support\n",
    "    # [CLS] and [SEP] as BERT need\n",
    "    tags_vals.append('X')\n",
    "    tags_vals.append('[CLS]')\n",
    "    tags_vals.append('[SEP]')\n",
    "    tags_vals.append(\"PAD\")\n",
    "\n",
    "    tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "    idx2tag = {i: t for i, t in enumerate(tags_vals) }\n",
    "\n",
    "    print('tags_vals: ', tags_vals)\n",
    "    print('tag2idx: ', tag2idx)\n",
    "    print('idx2tag: ', idx2tag)\n",
    "\n",
    "    tag_values = tags_vals\n",
    "    \n",
    "    # Mapping tag to name\n",
    "    tag2name={tag2idx[key] : key for key in tag2idx.keys()}\n",
    "    print('tag2name: ', tag2name)\n",
    "\n",
    "    return tags_vals, tag_values, tag2idx, idx2tag, tag2name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_BIO_data_into_sentences(data):\n",
    "    #concat sentence\n",
    "    getter = SentenceGetter(data)\n",
    "    \n",
    "    sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
    "    sentences_sbw = [[s[0] for s in sent] for sent in getter.sentences]\n",
    "    labels = [[s[2] for s in sent] for sent in getter.sentences]\n",
    "    return sentences, sentences_sbw, labels\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "# TEXT TOKENIZATION and EXTENSION OF LABELS FOR SPLITTED TOKENS\n",
    "def tokenize_texts_extend_labels(sentences, labels):\n",
    "    tokenized_texts = []\n",
    "    tokenized_labels = []\n",
    "    for sent, labs in zip(sentences, labels):\n",
    "        tokenized_sentence = []\n",
    "        labels = []\n",
    "\n",
    "        sent_tokens = sent.split()\n",
    "        for word, label in zip(sent_tokens, labs):\n",
    "\n",
    "            # Tokenize the word and count # of subwords the word is broken into\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            n_subwords = len(tokenized_word)\n",
    "\n",
    "            # Add the tokenized word to the final tokenized word list\n",
    "            tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "            # Add the same label to the new list of labels `n_subwords` times\n",
    "            labels.extend([label] * n_subwords)\n",
    "\n",
    "        tokenized_texts.append(tokenized_sentence)\n",
    "        tokenized_labels.append(labels)\n",
    "    \n",
    "    return tokenized_texts, tokenized_labels\n",
    "\n",
    "# tokenized_texts, tokenized_labels = tokenize_texts_extend_labels(sentences, labels)\n",
    "\n",
    "def tokenize_texts_and_labels(tokenizer, max_len, sentences, labels, tag2idx):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "    #TOKENIZE TEXTS and LABELS:\n",
    "    tokenized_texts, tokenized_labels = tokenize_texts_extend_labels(sentences, labels)\n",
    "    \n",
    "    # SGD (added to comply with previous versions of code)\n",
    "    word_piece_labels = tokenized_labels\n",
    "\n",
    "    # INPUT IDs:\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                              maxlen=max_len, dtype=\"long\", value=0.0,\n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    # TAGS:\n",
    "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in tokenized_labels],\n",
    "                         maxlen=max_len, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                         dtype=\"long\", truncating=\"post\")\n",
    "\n",
    "    # ATTENTION MASKS:\n",
    "    attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "        \n",
    "        \n",
    "    return tokenized_texts, tokenized_labels, word_piece_labels, input_ids, tags, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Compare True Labels vs Predicted Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) (load sample data , tokenize, split train/validation dataset, convert to tensors, prepare dataloader) - execute only if you load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FOLDER = 'training-datasets/'\n",
    "filename = filename\n",
    "# filename = 'scw_24_49_da'\n",
    "# # filename = 'sapner_tds_836_1.csv'\n",
    "# # filename = 'scw_sup_annot_1-23.csv'\n",
    "# # filename = 'scw_sup_annot_1-23_v5.csv'\n",
    "# # filename = 'scw_41_50_da.csv'\n",
    "# filename = 'scw_01-23_sa_v6.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(BASE_FOLDER+filename,sep=\",\",encoding=\"latin1\").fillna(method='ffill')    \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag_values_tag2idx_idx2tag_tag2name(data)\n",
    "sentences, sentences_sbw, labels = turn_BIO_data_into_sentences(data)\n",
    "\n",
    "# Get tags_vals, tag_values, tag2idx, idx2tag, tag2name:\n",
    "# tags_vals, tag_values, tag2idx, idx2tag, tag2name = tag_values_tag2idx_idx2tag_tag2name_from_data(data)\n",
    "tags_vals, tag_values, tag2idx, idx2tag, tag2name = tag_values_tag2idx_idx2tag_tag2name_from_model(selected_model)\n",
    "    \n",
    "# Set GPUs \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "# TOKENIZE TEXTS AND LABELS:\n",
    "max_len = 256\n",
    "MAX_LEN = max_len\n",
    "model_max_length = max_len\n",
    "\n",
    "tokenized_texts, tokenized_labels, word_piece_labels, input_ids, tags, attention_masks = tokenize_texts_and_labels(tokenizer, max_len, sentences, labels, tag2idx)\n",
    "\n",
    "# n = 15\n",
    "# print(tokenized_texts[n])\n",
    "# print(tokenized_labels[n])\n",
    "# print(word_piece_labels[n])\n",
    "# print(input_ids[n])\n",
    "# print(tags[0])\n",
    "# print(attention_masks[0])\n",
    "\n",
    "# SPLIT TRAINING/ VALIDATION DATASET:\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
    "                                                            random_state=2018, test_size=0.2)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.2)\n",
    "\n",
    "# CONVERT TO TORCH TENSORS (since we are operating in Pytorch)\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "\n",
    "# SET BATCH-SIZE (BS): val_inputs, tag2name\n",
    "bs = 16\n",
    "batch_num = bs\n",
    "\n",
    "# DEFINE DATALOADERS: \n",
    "#We shuffle the data at training time with the RandomSampler \n",
    "# and at test time we just pass them sequentially with the SequentialSampler.\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Detokenize Tokens/Labels from given sentence of Training/Validation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.1)  detokenize sentence into tokens labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.1) detokenize sentence into tokens labels\n",
    "\n",
    "def labels_from_label_ids(label_ids, idx2tag):\n",
    "    label_names = []\n",
    "    for i in label_ids:\n",
    "        label = idx2tag[i]\n",
    "        label_names.append(label)  \n",
    "    return label_names\n",
    "\n",
    "def detokenize_sentence_into_tokens_labels(inputs_i, masks_i, tags_i, idx2tag):\n",
    "    # Tokens from inputs\n",
    "    id_list = inputs_i\n",
    "    tokns = tokenizer.convert_ids_to_tokens(id_list)\n",
    "    # Labels from Tags\n",
    "    label_ids = tags_i.tolist()\n",
    "    # Get labels from Label Ids\n",
    "    label_names = labels_from_label_ids(label_ids, idx2tag)\n",
    "    # Dataframe with Tokens/Labels\n",
    "    token_label_dict = {'Token': tokns, 'Label': label_names}\n",
    "    token_label_df_tmp = pd.DataFrame(token_label_dict)\n",
    "    token_label_df = token_label_df_tmp[token_label_df_tmp['Token'] != '[PAD]']\n",
    "    # generate list from 'token_label_df':\n",
    "    tkn_lbl = token_label_df.values.tolist()\n",
    "    # clean tokens: remove rows where Token contains '##'\n",
    "    new_df = token_label_df[~token_label_df.Token.str.contains(\"##\")]\n",
    "    # generate ist from 'new_df':\n",
    "    tkn_lbl_clean = new_df.values.tolist()\n",
    "    \n",
    "    return tkn_lbl, tkn_lbl_clean\n",
    "\n",
    "# UNIT TESTING:\n",
    "# # # # Execution\n",
    "# n = 15\n",
    "\n",
    "# inputs_i = val_inputs[n]\n",
    "# masks_i = val_masks[n]\n",
    "# tags_i = val_tags[n]\n",
    "\n",
    "# tkn_lbl, tkn_lbl_clean = detokenize_sentence_into_tokens_labels(inputs_i, masks_i, tags_i, idx2tag)\n",
    "# print('tkn_lbl: ', tkn_lbl)\n",
    "# print('-'*50)\n",
    "# print('tkn_lbl_clean: ', tkn_lbl_clean)\n",
    "# print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.2) Get entity List from Tokens/Labels list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.2) Get entity List from Tokens/Labels list\n",
    "\n",
    "def get_entities_from_token_label_list(tkn_lbl):\n",
    "    prev_lbl = 'O'\n",
    "    entity = ''\n",
    "    entity_list = []\n",
    "    space = ' '\n",
    "\n",
    "    # tkn_lbl = [['sa', 'B-PROD']]\n",
    "\n",
    "    for i in tkn_lbl:\n",
    "        if i[1] == 'B-PROD':\n",
    "            if prev_lbl == 'O' or prev_lbl == 'I-PROD':\n",
    "                if entity != '':\n",
    "                    entity_list.append(entity)\n",
    "                    entity = ''\n",
    "\n",
    "                tkn = i[0].replace('##','')\n",
    "                entity = tkn\n",
    "                prev_lbl = 'B-PROD'\n",
    "\n",
    "            elif prev_lbl == 'B-PROD':\n",
    "                tkn = i[0].replace('##','')\n",
    "                entity = entity + tkn\n",
    "                prev_lbl = 'B-PROD'         \n",
    "\n",
    "        elif i[1] == 'I-PROD':\n",
    "            if prev_lbl == 'B-PROD':\n",
    "                tkn = i[0].replace('##','')\n",
    "                entity = entity + space\n",
    "                entity = entity + tkn\n",
    "                prev_lbl = 'I-PROD'\n",
    "\n",
    "            elif prev_lbl == 'I-PROD':\n",
    "                if '##' in i[0]:\n",
    "                    tkn = i[0].replace('##','')\n",
    "                    entity = entity + tkn    \n",
    "                    prev_lbl = 'I-PROD'\n",
    "                elif '##' not in i[0]:\n",
    "                    tkn = i[0].replace('##','')\n",
    "                    entity = entity + space\n",
    "                    entity = entity + tkn    \n",
    "                    prev_lbl = 'I-PROD'\n",
    "                    \n",
    "            elif prev_lbl == 'O':\n",
    "                if entity != '':\n",
    "                    entity_list.append(entity)\n",
    "                    entity = ''\n",
    "\n",
    "                tkn = i[0].replace('##','')\n",
    "                entity = entity + tkn \n",
    "                prev_lbl = 'I-PROD'\n",
    "\n",
    "        elif i[1] == 'O':\n",
    "            if prev_lbl == 'B-PROD' or prev_lbl == 'I-PROD':\n",
    "                entity_list.append(entity)\n",
    "                entity = ''\n",
    "                prev_lbl = 'O'\n",
    "            elif prev_lbl == 'O':\n",
    "                prev_lbl = 'O'\n",
    "\n",
    "    if entity != '':\n",
    "        entity_list.append(entity)\n",
    "\n",
    "    return entity_list\n",
    "\n",
    "# UNIT TESTING:\n",
    "# # # Execution\n",
    "# entity_list = get_entities_from_token_label_list(tkn_lbl)\n",
    "# print('entity_list: ', entity_list)\n",
    "# print('tkn_lbl: ', tkn_lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.3) Return list with separated fields for Tokens/Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_list_with_separated_fields_for_Tokens_Labels(tkn_lbl):\n",
    "\n",
    "    tkn_lbl_joined = []\n",
    "    tkns = []\n",
    "    lbls = []\n",
    "\n",
    "    for i in tkn_lbl:\n",
    "        tkns.append(i[0])\n",
    "        lbls.append(i[1])\n",
    "\n",
    "    # print(tkns)\n",
    "    # print(lbls)\n",
    "\n",
    "    tkn_lbl_joined.append(tkns)\n",
    "    tkn_lbl_joined.append(lbls)\n",
    "    \n",
    "    return tkns, lbls, tkn_lbl_joined\n",
    "\n",
    "# ## Execution\n",
    "# tkns, lbls, tkn_lbl_joined = return_list_with_separated_fields_for_Tokens_Labels(tkn_lbl)\n",
    "\n",
    "# print('tkns: ', tkns)\n",
    "# print('-'*50)\n",
    "# print('lbls: ',lbls )\n",
    "# print('-'*50)\n",
    "# print('tkn_lbl_joined: ',tkn_lbl_joined)\n",
    "# print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.4) Return clean list of Tokens/Labels (joining splitted words '##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_clean_list_of_tokens_labels(tkn_lbl_clean, tkns):\n",
    "    # Get final list of labels\n",
    "    final_labels = []\n",
    "    for i in tkn_lbl_clean:\n",
    "        final_labels.append(i[1])\n",
    "    \n",
    "    # Get final list of Tokens\n",
    "    txt = ' '.join([x for x in tkns])\n",
    "    fine_text = txt.replace(' ##', '')\n",
    "    fine_text = fine_text.split()\n",
    "\n",
    "    final_tokens = []\n",
    "    for i in fine_text:\n",
    "        final_tokens.append(i)\n",
    "        \n",
    "    # Compose output dataframe\n",
    "    token_label_dict2 = {'Token': final_tokens, 'Label': final_labels}\n",
    "    token_label_final = pd.DataFrame(token_label_dict2)\n",
    "    \n",
    "    return token_label_final, final_tokens, final_labels\n",
    "\n",
    "# UNIT TESTING:\n",
    "# # # Execution\n",
    "# token_label_final, final_tokens, final_labels = return_clean_list_of_tokens_labels(tkn_lbl_clean, tkns)\n",
    "# print(token_label_final)\n",
    "# print(final_tokens)\n",
    "# print(final_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.5) From Tain/Valid sentence to tokens/labels (1 shot execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_tokens_labels_from_train_validation_sentence(n, inputs, masks, tags):\n",
    "    inputs_i = inputs\n",
    "    masks_i = masks\n",
    "    tags_i = tags\n",
    "\n",
    "    tkn_lbl, tkn_lbl_clean = detokenize_sentence_into_tokens_labels(inputs_i, masks_i, tags_i, idx2tag)\n",
    "\n",
    "    entity_list = get_entities_from_token_label_list(tkn_lbl)\n",
    "\n",
    "    tkns, lbls, tkn_lbl_joined = return_list_with_separated_fields_for_Tokens_Labels(tkn_lbl)\n",
    "\n",
    "    token_label_final, final_tokens, final_labels = return_clean_list_of_tokens_labels(tkn_lbl_clean, tkns)\n",
    "    \n",
    "    return token_label_final, final_tokens, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNITE TEST INFERENCE\n",
    "sentce = 'sap erp is the best product'\n",
    "inference_sap_bert(sentce, model)\n",
    "\n",
    "inference_with_prob(sentce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.6) GET TRUE/PREDICTED LABELS FOR EVERY SENTENCE IN TRAIN/VALIDATION DATASET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET TRUE/PREDICTED LABELS FOR EVERY SENTENCE IN TRAIN/VALIDATION DATASET:\n",
    "\n",
    "#--- set parameters:\n",
    "take_from = 'validation_dataset'\n",
    "# take_from = 'training_dataset'\n",
    "\n",
    "#--- execute\n",
    "true_pred_final_df = pd.DataFrame()\n",
    "counter = 1\n",
    "\n",
    "for n in range(0, len(val_inputs)):\n",
    "    # take the right dataset based on parameter 'take_from':\n",
    "    if take_from == 'validation_dataset':\n",
    "        # a) take from 'Validation data'\n",
    "        inputs_i = val_inputs[n]\n",
    "        masks_i = val_masks[n]\n",
    "        tags_i = val_tags[n]\n",
    "\n",
    "    elif take_from == 'training_dataset':\n",
    "        # b) take from 'Training data'\n",
    "        inputs_i = tr_inputs[n]\n",
    "        masks_i = tr_masks[n]\n",
    "        tags_i = tr_tags[n]\n",
    "    \n",
    "    # detokenize sentence\n",
    "    token_label_final, final_tokens, final_labels = final_tokens_labels_from_train_validation_sentence(n, inputs_i, masks_i, tags_i)\n",
    "    \n",
    "    # dataframe with 'true' token/labels of that sentence\n",
    "    s_true = token_label_final\n",
    "    s_true  \n",
    "    \n",
    "    # compose sentence from tokens\n",
    "    sentce = ''\n",
    "    space = ' '\n",
    "\n",
    "    for i in final_tokens:\n",
    "        sentce = sentce + i\n",
    "        sentce = sentce + space\n",
    "        \n",
    "    # Inference sentence (predict labels based on trained model)\n",
    "    s_pred = inference_sap_bert(sentce, model)\n",
    "    s_pred = s_pred.reset_index(drop=True)\n",
    "    \n",
    "    # join 's_true' and 's_pred' dataframes. Add Sentence column\n",
    "    s_true_pred_df = token_label_final.join(s_pred, lsuffix='true_labels', rsuffix='pred_labels')\n",
    "    s_true_pred_df['Sentence'] = counter\n",
    "    counter = counter + 1\n",
    "    \n",
    "    # add current 's_true_pred_df' into 'true_pred_final_df' \n",
    "    df1 = true_pred_final_df.copy()\n",
    "    df2 = s_true_pred_df\n",
    "\n",
    "    frames = [df1, df2]\n",
    "\n",
    "    true_pred_final_df = pd.concat(frames)\n",
    "    true_pred_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pred_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE to .CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_data = 'tr_da_1_23_val_sa'\n",
    "# model_data = filename_main\n",
    "\n",
    "file_path_to_a = \"models_inference_true_pred/\"+model_name+filename_main+\"_true_pred.csv\"\n",
    "true_pred_final_df.to_csv(file_path_to_a, sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_to_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE Inference Parameters to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_datetime():\n",
    "    # using time module \n",
    "    import time \n",
    "    from datetime import datetime\n",
    "\n",
    "    # ts stores the time in seconds \n",
    "    ts = time.time() \n",
    "    # print(ts) \n",
    "\n",
    "    #convert timestamp to date/time\n",
    "    dt_object = datetime.fromtimestamp(ts)\n",
    "    # print(\"dt_object =\", dt_object)\n",
    "    # print(\"type(dt_object) =\", type(dt_object))\n",
    "\n",
    "    #get datetime\n",
    "    datetime = str(dt_object)\n",
    "    datetime = datetime[:19]\n",
    "\n",
    "    return datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE Inference Parameters and Evaluation Results\n",
    "import os\n",
    "BASE_FOLDER = \"models_inference_true_pred/\"\n",
    "\n",
    "file = os.path.join(BASE_FOLDER, model_name+'_parameters')\n",
    "with open(file, \"w\") as writer:\n",
    "    writer.write('\\n--------------------------------------------------------------')\n",
    "    writer.write(\"\\nExecution Parameters:   \")\n",
    "    writer.write('\\n--------------------------------------------------------------')\n",
    "    writer.write(\"\\nDatetime: \") \n",
    "    writer.write(get_current_datetime())\n",
    "    writer.write(\"\\nModel Inferenced:  \") \n",
    "    writer.write(model_name)\n",
    "    writer.write(\"\\nSource data:  \") \n",
    "    writer.write(filename)\n",
    "    writer.write(\"\\nData split used:  \") \n",
    "    writer.write(take_from )\n",
    "    writer.write('\\n--------------------------------------------------------------')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Inferenced: \", model_name) \n",
    "print(\"Source data: \" , filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution (FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execution (final):\n",
    "\n",
    "# n=15\n",
    "\n",
    "# take_from = 'validation_dataset'\n",
    "# # take_from = 'training_dataset'\n",
    "\n",
    "# if take_from == 'validation_dataset':\n",
    "#     # a) take from 'Validation data'\n",
    "#     inputs_i = val_inputs[n]\n",
    "#     masks_i = val_masks[n]\n",
    "#     tags_i = val_tags[n]\n",
    "\n",
    "# elif take_from == 'training_dataset':\n",
    "#     # b) take from 'Training data'\n",
    "#     inputs_i = tr_inputs[n]\n",
    "#     masks_i = tr_masks[n]\n",
    "#     tags_i = tr_tags[n]\n",
    "\n",
    "# token_label_final, final_tokens, final_labels = final_tokens_labels_from_train_validation_sentence(n, inputs_i, masks_i, tags_i)\n",
    "\n",
    "# # b) take from 'Training data'\n",
    "# # token_label_final = final_tokens_labels_from_train_validation_sentence(n, tr_inputs, tr_masks, tr_tags)\n",
    "\n",
    "# # Final result\n",
    "# s_true = token_label_final\n",
    "# s_true"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.4 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.4-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
